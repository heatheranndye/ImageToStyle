{"cells":[{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision.utils import save_image\n","import torch.optim as optim\n","import pathlib\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["DATAPATH = pathlib.Path.cwd() / \"data\"\n","\n","model = models.vgg19(weights=\"IMAGENET1K_V1\").features\n","device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["CONTENT_PATH =DATAPATH / \"squareflower.jpg\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["STYLE_PATH = DATAPATH /\"picasso.jpg\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def image_loader(path):\n","    image = Image.open(path)\n","    loader = transforms.Compose(\n","        [\n","            transforms.Resize(\n","                (\n","                    256,\n","                    256,\n","                )\n","            ),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device, torch.float)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["content_img = image_loader(CONTENT_PATH) \n","style_img = image_loader(STYLE_PATH)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class VGG(nn.Module):\n","    def __init__(self):\n","        super(VGG, self).__init__()\n","        self.req_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n","        # Since we need only the 5 layers in the model so we\n","        #  will be dropping all the rest layers from the features of the model\n","        self.model = models.vgg19(weights=\"IMAGENET1K_V1\").features[\n","            :29\n","        ]  # model will contain the first 29 layers\n","\n","    # x holds the input tensor(image) that will be feeded to each layer\n","    def forward(self, x):\n","        # initialize an array that wil hold the\n","        # activations from the chosen layers\n","        features = []\n","        # Iterate over all the layers of the mode\n","        for layer_num, layer in enumerate(self.model):\n","            # activation of the layer will stored in x\n","            x = layer(x)\n","            # appending the activation of the selected\n","            #  layers and return the feature array\n","            if str(layer_num) in self.req_features[0:]:\n","                features.append(x)\n","                \n","        return features\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Load the model to the GPU\n","model = VGG().to(device).eval()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["style_feat = model(style_img)\n","cont_feat=model(content_img)\n","gen_image = content_img.clone().detach().requires_grad_(True)\n","gen_feat = model(gen_image)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(style_feat)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def calc_style_loss(gen, style):\n","    # Calculating the gram matrix for the style and the generated image\n","    style_l=0\n","    for x in range(0,5,1):\n","        #print(x)\n","        batch, channel, height, width = gen[x].shape\n","        genitem = gen[x]\n","        styleitem = style[x]\n","        G = torch.mm(\n","            genitem.view(\n","             channel,\n","            height * width,\n","        ),\n","        genitem.view(channel, height * width).t(),\n","        )\n","        A = torch.mm(\n","        styleitem.view(\n","            channel,\n","            height * width,\n","        ),\n","        styleitem.view(channel, height * width).t(),\n","        )\n","\n","    # Calcultating the style loss of each layer by\n","    # calculating the MSE between the gram matrix of\n","    # the style image and the generated image and adding it to style loss\n","        style_l += F.mse_loss(G,A)\n","    return style_l\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen_feat[3].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test1 = calc_style_loss(gen_feat, style_feat)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test1"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def calc_content_loss(gen_feat, orig_feat):\n","    # calculating the content loss of each layer\n","    #  by calculating the MSE between the content and\n","    #  generated features and adding it to content loss\n","    content_l = 0\n","    x=4\n","    #for x in range(len(gen_feat)):\n","    content_l += F.mse_loss(gen_feat[x], orig_feat[x])\n","    return content_l\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = calc_content_loss(gen_feat, cont_feat)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test2 = calc_style_loss(gen_feat, style_feat)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["alpha = 1000\n","beta = 10"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def calculate_loss(\n","    gen,\n","    cont,\n","    style,\n","):\n","    style_loss = content_loss = 0\n","        # extracting the dimensions from the generated image\n","    content_loss += calc_content_loss(gen, cont)\n","    style_loss += calc_style_loss(gen, style)\n","\n","    # calculating the total loss of e th epoch\n","    total_loss = alpha * content_loss + beta * style_loss\n","    return total_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test3 = calculate_loss(gen_feat,cont_feat,style_feat)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test3"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["epoch = 1000\n","lr = 0.04\n","alpha = 1000\n","beta = 50\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def image_trainer() -> Image:\n","    original_image = image_loader(DATAPATH / \"squareflower.jpg\")\n","    style_image = image_loader(DATAPATH / \"YellowFlowerWaterPixelTrim.jpg\")\n","    image_name = \"gen5.jpg\"\n","    generated_image = original_image.clone().requires_grad_(True)\n","    model.requires_grad_(False)\n","    # using adam optimizer and it will update\n","    #  the generated image not the model parameter\n","    optimizer = optim.Adam([generated_image], lr=lr)\n","\n","    # iterating for 1000 times\n","    for e in range(100):\n","        # extracting the features of generated, content\n","        # and the original required for calculating the loss\n","        gen_features = model(generated_image)\n","        orig_features = model(original_image)\n","        style_features = model(style_image)\n","        \n","        # iterating over the activation of each layer\n","        # and calculate the loss and add it to the content and the style loss\n","        total_loss = calculate_loss(\n","            gen_features,\n","            orig_features,\n","            style_features,\n","        )\n","        # optimize the pixel values of the generated\n","        # image and backpropagate the loss\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        optimizer.step()\n","        # print the image and save it after each 100 epoch\n","        if e / 100:\n","            print(total_loss)\n","\n","            save_image(generated_image, image_name)\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(6.1845e+08, grad_fn=<AddBackward0>)\n","tensor(5.7175e+08, grad_fn=<AddBackward0>)\n","tensor(6.0897e+08, grad_fn=<AddBackward0>)\n","tensor(5.8398e+08, grad_fn=<AddBackward0>)\n","tensor(5.3986e+08, grad_fn=<AddBackward0>)\n","tensor(4.8806e+08, grad_fn=<AddBackward0>)\n","tensor(4.3959e+08, grad_fn=<AddBackward0>)\n","tensor(3.9659e+08, grad_fn=<AddBackward0>)\n","tensor(3.5789e+08, grad_fn=<AddBackward0>)\n","tensor(3.2335e+08, grad_fn=<AddBackward0>)\n","tensor(2.9241e+08, grad_fn=<AddBackward0>)\n","tensor(2.6518e+08, grad_fn=<AddBackward0>)\n","tensor(2.4060e+08, grad_fn=<AddBackward0>)\n","tensor(2.1888e+08, grad_fn=<AddBackward0>)\n","tensor(1.9926e+08, grad_fn=<AddBackward0>)\n","tensor(1.8143e+08, grad_fn=<AddBackward0>)\n","tensor(1.6574e+08, grad_fn=<AddBackward0>)\n","tensor(1.5220e+08, grad_fn=<AddBackward0>)\n","tensor(1.4031e+08, grad_fn=<AddBackward0>)\n","tensor(1.2949e+08, grad_fn=<AddBackward0>)\n","tensor(1.1944e+08, grad_fn=<AddBackward0>)\n","tensor(1.1055e+08, grad_fn=<AddBackward0>)\n","tensor(1.0395e+08, grad_fn=<AddBackward0>)\n","tensor(98669304., grad_fn=<AddBackward0>)\n","tensor(97228136., grad_fn=<AddBackward0>)\n","tensor(84431232., grad_fn=<AddBackward0>)\n","tensor(79859672., grad_fn=<AddBackward0>)\n","tensor(76584712., grad_fn=<AddBackward0>)\n","tensor(68996424., grad_fn=<AddBackward0>)\n","tensor(63921452., grad_fn=<AddBackward0>)\n","tensor(60310012., grad_fn=<AddBackward0>)\n","tensor(56306916., grad_fn=<AddBackward0>)\n","tensor(52573548., grad_fn=<AddBackward0>)\n","tensor(48544312., grad_fn=<AddBackward0>)\n","tensor(45109676., grad_fn=<AddBackward0>)\n","tensor(41839648., grad_fn=<AddBackward0>)\n","tensor(38870440., grad_fn=<AddBackward0>)\n","tensor(36175232., grad_fn=<AddBackward0>)\n","tensor(33742568., grad_fn=<AddBackward0>)\n","tensor(31567246., grad_fn=<AddBackward0>)\n","tensor(29573090., grad_fn=<AddBackward0>)\n","tensor(27712248., grad_fn=<AddBackward0>)\n","tensor(25994148., grad_fn=<AddBackward0>)\n","tensor(24462420., grad_fn=<AddBackward0>)\n","tensor(23376916., grad_fn=<AddBackward0>)\n","tensor(24468062., grad_fn=<AddBackward0>)\n","tensor(28942138., grad_fn=<AddBackward0>)\n","tensor(35559268., grad_fn=<AddBackward0>)\n","tensor(22717768., grad_fn=<AddBackward0>)\n","tensor(30375896., grad_fn=<AddBackward0>)\n","tensor(31880430., grad_fn=<AddBackward0>)\n","tensor(24899264., grad_fn=<AddBackward0>)\n","tensor(29319666., grad_fn=<AddBackward0>)\n","tensor(29084930., grad_fn=<AddBackward0>)\n","tensor(24462094., grad_fn=<AddBackward0>)\n","tensor(26657624., grad_fn=<AddBackward0>)\n","tensor(25036074., grad_fn=<AddBackward0>)\n","tensor(22452440., grad_fn=<AddBackward0>)\n","tensor(22715398., grad_fn=<AddBackward0>)\n","tensor(21203562., grad_fn=<AddBackward0>)\n","tensor(19320454., grad_fn=<AddBackward0>)\n","tensor(18980960., grad_fn=<AddBackward0>)\n","tensor(18043592., grad_fn=<AddBackward0>)\n","tensor(16527450., grad_fn=<AddBackward0>)\n","tensor(15923297., grad_fn=<AddBackward0>)\n","tensor(15531920., grad_fn=<AddBackward0>)\n","tensor(14603331., grad_fn=<AddBackward0>)\n","tensor(13816273., grad_fn=<AddBackward0>)\n","tensor(13273227., grad_fn=<AddBackward0>)\n","tensor(12907396., grad_fn=<AddBackward0>)\n","tensor(12665246., grad_fn=<AddBackward0>)\n","tensor(12476780., grad_fn=<AddBackward0>)\n","tensor(12488524., grad_fn=<AddBackward0>)\n","tensor(12004760., grad_fn=<AddBackward0>)\n","tensor(11489146., grad_fn=<AddBackward0>)\n","tensor(10848966., grad_fn=<AddBackward0>)\n","tensor(10689342., grad_fn=<AddBackward0>)\n","tensor(10693893., grad_fn=<AddBackward0>)\n","tensor(10436670., grad_fn=<AddBackward0>)\n","tensor(10118549., grad_fn=<AddBackward0>)\n","tensor(9779334., grad_fn=<AddBackward0>)\n","tensor(9563953., grad_fn=<AddBackward0>)\n","tensor(9460583., grad_fn=<AddBackward0>)\n","tensor(9409017., grad_fn=<AddBackward0>)\n","tensor(9529972., grad_fn=<AddBackward0>)\n","tensor(9729617., grad_fn=<AddBackward0>)\n","tensor(10113978., grad_fn=<AddBackward0>)\n","tensor(9131773., grad_fn=<AddBackward0>)\n","tensor(8858265., grad_fn=<AddBackward0>)\n","tensor(8874058., grad_fn=<AddBackward0>)\n","tensor(8732235., grad_fn=<AddBackward0>)\n","tensor(8762284., grad_fn=<AddBackward0>)\n","tensor(8291146., grad_fn=<AddBackward0>)\n","tensor(8417589., grad_fn=<AddBackward0>)\n","tensor(8316583., grad_fn=<AddBackward0>)\n","tensor(8319488.5000, grad_fn=<AddBackward0>)\n","tensor(8343964.5000, grad_fn=<AddBackward0>)\n","tensor(8077166., grad_fn=<AddBackward0>)\n","tensor(8070335.5000, grad_fn=<AddBackward0>)\n"]}],"source":["try1 = image_trainer()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"styletransfer","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"f22c370e7f3f675494fd6af0987d18d1bd52cf49e26db54bd88c6c515a554e22"}}},"nbformat":4,"nbformat_minor":2}
