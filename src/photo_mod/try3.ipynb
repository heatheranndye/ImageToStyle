{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision.utils import save_image\n","import torch.optim as optim\n","import pathlib\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","\n","DATAPATH = pathlib.Path.cwd()/ \"data\"\n","\n","model = models.vgg19(weights=\"IMAGENET1K_V1\").features\n","device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["CONTENT = DATAPATH / \"squareflower.jpg\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","\n","def image_loader(path):\n","    image = Image.open(path)\n","    loader = transforms.Compose(\n","        [\n","            transforms.Resize(\n","                (\n","                    128,\n","                    128,\n","                )\n","            ),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device, torch.float)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["cont_img = image_loader(CONTENT)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","\n","# Defining a class that for the model\n","\n","\n","class VGG(nn.Module):\n","    def __init__(self):\n","        super(VGG, self).__init__()\n","        self.req_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n","        # Since we need only the 5 layers in the model so we\n","        #  will be dropping all the rest layers from the features of the model\n","        self.model = models.vgg19(weights=\"IMAGENET1K_V1\").features[\n","            :29\n","        ]  # model will contain the first 29 layers\n","\n","    # x holds the input tensor(image) that will be feeded to each layer\n","    def forward(self, x):\n","        # initialize an array that wil hold the\n","        # activations from the chosen layers\n","        features = []\n","        # Iterate over all the layers of the mode\n","        for layer_num, layer in enumerate(self.model):\n","            # activation of the layer will stored in x\n","            x = layer(x)\n","            # appending the activation of the selected\n","            #  layers and return the feature array\n","            if str(layer_num) in self.req_features:\n","                features.append(x)\n","                print(f\"x shape{x.shape}\")\n","\n","        return features\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["model = VGG().to(device).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = optim.Adam([generated_image], lr=lr)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["cont_feat = model(cont_img)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 512, 4, 4])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["cont_feat.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def calc_content_loss(gen_feat, orig_feat):\n","    # calculating the content loss of each layer\n","    #  by calculating the MSE between the content and\n","    #  generated features and adding it to content loss\n","    content_l = torch.mean((gen_feat - orig_feat) ** 2)\n","    return content_l\n","\n","\n","def calc_style_loss(gen, style):\n","    # Calculating the gram matrix for the style and the generated image\n","    batch_size, channel, height, width = gen.shape\n","\n","    G = torch.mm(\n","        gen.view(\n","            channel,\n","            height * width,\n","        ),\n","        gen.view(channel, height * width).t(),\n","    )\n","    A = torch.mm(\n","        style.view(\n","            channel,\n","            height * width,\n","        ),\n","        style.view(channel, height * width).t(),\n","    )\n","\n","    # Calcultating the style loss of each layer by\n","    # calculating the MSE between the gram matrix of\n","    # the style image and the generated image and adding it to style loss\n","    style_l = torch.mean((G - A) ** 2)\n","    return style_l\n","\n","\n","def calculate_loss(\n","    gen_features,\n","    orig_feautes,\n","    style_featues,\n","):\n","    style_loss = content_loss = 0\n","    for gen, cont, style in zip(\n","        gen_features,\n","        orig_feautes,\n","        style_featues,\n","    ):\n","        # extracting the dimensions from the generated image\n","        content_loss += calc_content_loss(gen, cont)\n","        style_loss += calc_style_loss(gen, style)\n","\n","    # calculating the total loss of e th epoch\n","    total_loss = alpha * content_loss + beta * style_loss\n","    return total_loss\n","\n","\n","# Load the model to the GPU\n","model = VGG().to(device).eval()\n","\n","# initialize the paramerters required for fitting the model\n","epoch = 1000\n","lr = 0.04\n","alpha = 10\n","beta = 50\n","\n","\n","def image_trainer(label: int) -> Image:\n","    original_image = image_loader(DATAPATH / \"flower3.jpg\")\n","    style_image = image_loader(DATAPATH / \"texture4.jpg\")\n","    image_name = \"gen\" + str(label) + \".jpg\"\n","    generated_image = original_image.clone().requires_grad_(True)\n","\n","    # using adam optimizer and it will update\n","    #  the generated image not the model parameter\n","    optimizer = optim.Adam([generated_image], lr=lr)\n","\n","    # iterating for 1000 times\n","    for e in range(100):\n","        # extracting the features of generated, content\n","        # and the original required for calculating the loss\n","        gen_features = model(generated_image)\n","        orig_feautures = model(original_image)\n","        style_features = model(style_image)\n","\n","        # iterating over the activation of each layer\n","        # and calculate the loss and add it to the content and the style loss\n","        total_loss = calculate_loss(\n","            gen_features,\n","            orig_feautures,\n","            style_features,\n","        )\n","        # optimize the pixel values of the generated\n","        # image and backpropagate the loss\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        optimizer.step()\n","        # print the image and save it after each 100 epoch\n","        if e / 100:\n","            print(total_loss)\n","\n","            save_image(generated_image, image_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATAPATH = pathlib.Path(__file__).parent / \"data\"\n","\n","model = models.vgg19(weights=\"IMAGENET1K_V1\").features\n","device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CONTENT_PATH = DATAPATH + \"squareflower.jpg\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CONTENT_PATH = DATAPATH / \"squareflower.jpg\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def image_loader(path):\n","    image = Image.open(path)\n","    loader = transforms.Compose(\n","        [\n","            transforms.Resize(\n","                (\n","                    512,\n","                    512,\n","                )\n","            ),\n","            transforms.ToTensor(),\n","        ]\n","    )\n","    image = loader(image).unsqueeze(0)\n","    return image.to(device, torch.float)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cont_img = image_loader(CONTENT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VGG(nn.Module):\n","    def __init__(self):\n","        super(VGG, self).__init__()\n","        self.req_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n","        # Since we need only the 5 layers in the model so we\n","        #  will be dropping all the rest layers from the features of the model\n","        self.model = models.vgg19(weights=\"IMAGENET1K_V1\").features[\n","            :29\n","        ]  # model will contain the first 29 layers\n","\n","    # x holds the input tensor(image) that will be feeded to each layer\n","    def forward(self, x):\n","        # initialize an array that wil hold the\n","        # activations from the chosen layers\n","        features = []\n","        # Iterate over all the layers of the mode\n","        for layer_num, layer in enumerate(self.model):\n","            # activation of the layer will stored in x\n","            x = layer(x)\n","            # appending the activation of the selected\n","            #  layers and return the feature array\n","            if str(layer_num) in self.req_features:\n","                features.append(x)\n","\n","        return features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generated_image = cont_img.clone().requires_grad_(True)\n","\n","    # using adam optimizer and it will update\n","    #  the generated image not the model parameter\n","optimizer = optim.Adam([generated_image], lr=lr)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen_features = model(generated_image)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(gen_features)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["modelc = VGG().to(device).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen_feat = modelc(cont_img)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(gen_feat)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["loss = nn.MSELoss()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["input = torch.randn(128,128, requires_grad=True)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([128, 128])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["input.shape"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["target = torch.randn(128,128)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([128, 128])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["target.shape"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["output = loss(input, target)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(2.0255, grad_fn=<MseLossBackward0>)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["output"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["output.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"styletransfer","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"f22c370e7f3f675494fd6af0987d18d1bd52cf49e26db54bd88c6c515a554e22"}}},"nbformat":4,"nbformat_minor":2}
